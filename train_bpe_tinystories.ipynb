{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error importing BPE components: No module named 'regex'\n",
      "Please ensure you're running this script from the assignment1-basics directory\n",
      "and that all required dependencies are installed.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 1\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "BPE Training Script for TinyStories Dataset\n",
    "\n",
    "This script trains a byte-level BPE tokenizer on the TinyStories dataset\n",
    "with the following specifications:\n",
    "- Maximum vocabulary size: 10,000\n",
    "- Special token: <|endoftext|>\n",
    "- Performance requirements: ≤30 minutes, ≤30GB RAM\n",
    "\n",
    "The script provides comprehensive analysis including:\n",
    "- Training time and memory usage\n",
    "- Performance profiling to identify bottlenecks\n",
    "- Vocabulary analysis including longest token\n",
    "- Serialization of results to disk\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import cProfile\n",
    "import pstats\n",
    "import psutil\n",
    "import tracemalloc\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Dict, List\n",
    "\n",
    "assignment_dir = \"/Users/shanshutong/Desktop/coarse/cs336/assignment1-basics\"\n",
    "os.chdir(assignment_dir)\n",
    "\n",
    "try:\n",
    "    # Import the BPE components directly\n",
    "    from tests.custom.bpe_counter import read_tokens, BPECounter\n",
    "\n",
    "    def run_train_bpe(input_path, vocab_size, special_tokens):\n",
    "        \"\"\"Local implementation of run_train_bpe to avoid import issues\"\"\"\n",
    "        tokens = read_tokens(input_path, special_tokens)\n",
    "        bpe_counter = BPECounter(tokens, vocab_size, special_tokens)\n",
    "        bpe_counter.merge()\n",
    "        return bpe_counter.vocab, bpe_counter.merges\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing BPE components: {e}\")\n",
    "    print(\"Please ensure you're running this script from the assignment1-basics directory\")\n",
    "    print(\"and that all required dependencies are installed.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "class MemoryMonitor:\n",
    "    \"\"\"Monitor memory usage during training\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.process = psutil.Process()\n",
    "        self.peak_memory = 0\n",
    "        self.start_memory = 0\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"Start monitoring memory\"\"\"\n",
    "        tracemalloc.start()\n",
    "        self.start_memory = self.process.memory_info().rss / 1024 / 1024  # MB\n",
    "        self.peak_memory = self.start_memory\n",
    "        print(f\"Initial memory usage: {self.start_memory:.2f} MB\")\n",
    "\n",
    "    def update_peak(self):\n",
    "        \"\"\"Update peak memory usage\"\"\"\n",
    "        current_memory = self.process.memory_info().rss / 1024 / 1024  # MB\n",
    "        if current_memory > self.peak_memory:\n",
    "            self.peak_memory = current_memory\n",
    "        return current_memory\n",
    "\n",
    "    def get_stats(self):\n",
    "        \"\"\"Get memory statistics\"\"\"\n",
    "        current_memory = self.process.memory_info().rss / 1024 / 1024  # MB\n",
    "        current, peak = tracemalloc.get_traced_memory()\n",
    "        tracemalloc.stop()\n",
    "\n",
    "        return {\n",
    "            'start_memory_mb': self.start_memory,\n",
    "            'current_memory_mb': current_memory,\n",
    "            'peak_memory_mb': self.peak_memory,\n",
    "            'tracemalloc_current_mb': current / 1024 / 1024,\n",
    "            'tracemalloc_peak_mb': peak / 1024 / 1024\n",
    "        }\n",
    "\n",
    "\n",
    "def save_tokenizer_results(vocab: Dict[int, bytes], merges: List[Tuple[bytes, bytes]],\n",
    "                          output_dir: str = \"bpe_results\") -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Save vocabulary and merges to disk\n",
    "\n",
    "    Args:\n",
    "        vocab: Vocabulary mapping from token ID to bytes\n",
    "        merges: List of BPE merge rules\n",
    "        output_dir: Directory to save results\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (vocab_path, merges_path)\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    Path(output_dir).mkdir(exist_ok=True)\n",
    "\n",
    "    # Save vocabulary as JSON (convert bytes to list of ints for serialization)\n",
    "    vocab_path = os.path.join(output_dir, \"vocab.json\")\n",
    "    vocab_serializable = {str(k): list(v) for k, v in vocab.items()}\n",
    "\n",
    "    with open(vocab_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(vocab_serializable, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    # Save merges as pickle (preserves bytes type)\n",
    "    merges_path = os.path.join(output_dir, \"merges.pkl\")\n",
    "    with open(merges_path, 'wb') as f:\n",
    "        pickle.dump(merges, f)\n",
    "\n",
    "    print(f\"Vocabulary saved to: {vocab_path}\")\n",
    "    print(f\"Merges saved to: {merges_path}\")\n",
    "\n",
    "    return vocab_path, merges_path\n",
    "\n",
    "\n",
    "def analyze_vocabulary(vocab: Dict[int, bytes]) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze the trained vocabulary\n",
    "\n",
    "    Args:\n",
    "        vocab: Vocabulary mapping from token ID to bytes\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with analysis results\n",
    "    \"\"\"\n",
    "    # Find longest token\n",
    "    longest_token = max(vocab.values(), key=len)\n",
    "    longest_token_id = [k for k, v in vocab.items() if v == longest_token][0]\n",
    "\n",
    "    # Analyze token lengths\n",
    "    token_lengths = [len(token) for token in vocab.values()]\n",
    "    avg_length = sum(token_lengths) / len(token_lengths)\n",
    "\n",
    "    # Try to decode longest token for readability analysis\n",
    "    try:\n",
    "        longest_token_str = longest_token.decode('utf-8', errors='replace')\n",
    "        is_readable = longest_token_str.isprintable()\n",
    "    except:\n",
    "        longest_token_str = str(longest_token)\n",
    "        is_readable = False\n",
    "\n",
    "    analysis = {\n",
    "        'vocab_size': len(vocab),\n",
    "        'longest_token': longest_token,\n",
    "        'longest_token_id': longest_token_id,\n",
    "        'longest_token_length': len(longest_token),\n",
    "        'longest_token_str': longest_token_str,\n",
    "        'longest_token_readable': is_readable,\n",
    "        'avg_token_length': avg_length,\n",
    "        'min_token_length': min(token_lengths),\n",
    "        'max_token_length': max(token_lengths)\n",
    "    }\n",
    "\n",
    "    return analysis\n",
    "\n",
    "\n",
    "def profile_bpe_training(input_path: str, vocab_size: int, special_tokens: List[str]) -> Tuple:\n",
    "    \"\"\"\n",
    "    Train BPE tokenizer with comprehensive profiling\n",
    "\n",
    "    Args:\n",
    "        input_path: Path to training data\n",
    "        vocab_size: Target vocabulary size\n",
    "        special_tokens: List of special tokens\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (vocab, merges, training_time, memory_stats, profile_stats)\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"STARTING BPE TRAINING ON TINYSTORIES DATASET\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Input file: {input_path}\")\n",
    "    print(f\"Target vocabulary size: {vocab_size}\")\n",
    "    print(f\"Special tokens: {special_tokens}\")\n",
    "    print()\n",
    "\n",
    "    # Initialize monitoring\n",
    "    memory_monitor = MemoryMonitor()\n",
    "    memory_monitor.start()\n",
    "\n",
    "    # Setup profiler\n",
    "    profiler = cProfile.Profile()\n",
    "\n",
    "    # Start training\n",
    "    print(\"Starting BPE training...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    profiler.enable()\n",
    "    try:\n",
    "        vocab, merges = run_train_bpe(\n",
    "            input_path=input_path,\n",
    "            vocab_size=vocab_size,\n",
    "            special_tokens=special_tokens\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        profiler.disable()\n",
    "\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    # Get memory statistics\n",
    "    memory_stats = memory_monitor.get_stats()\n",
    "\n",
    "    # Process profiling results\n",
    "    stats = pstats.Stats(profiler)\n",
    "    stats.sort_stats('cumulative')\n",
    "\n",
    "    print(f\"\\nTraining completed successfully!\")\n",
    "    print(f\"Training time: {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
    "    print(f\"Peak memory usage: {memory_stats['peak_memory_mb']:.2f} MB ({memory_stats['peak_memory_mb']/1024:.2f} GB)\")\n",
    "\n",
    "    return vocab, merges, training_time, memory_stats, stats\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run BPE training and analysis\"\"\"\n",
    "\n",
    "    # Configuration\n",
    "    input_path = \"/content/data/TinyStoriesV2-GPT4-valid.txt\"\n",
    "    vocab_size = 10000\n",
    "    special_tokens = [\"<|endoftext|>\"]\n",
    "    output_dir = \"bpe_results\"\n",
    "\n",
    "    # Check for test mode\n",
    "    if len(sys.argv) > 1 and sys.argv[1] == \"--test\":\n",
    "        print(\"Running in test mode with smaller dataset...\")\n",
    "        input_path = \"data/test_tinystories.txt\"\n",
    "        vocab_size = 500  # Smaller vocab for testing\n",
    "\n",
    "    # Check if input file exists\n",
    "    if not os.path.exists(input_path):\n",
    "        print(f\"Error: Input file not found: {input_path}\")\n",
    "        print(\"\\nPlease download the TinyStories dataset first:\")\n",
    "        print(\"mkdir -p data\")\n",
    "        print(\"cd data\")\n",
    "        print(\"wget https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-train.txt\")\n",
    "        print(\"cd ..\")\n",
    "        print(\"\\nAlternatively, you can use the validation file for testing:\")\n",
    "        print(\"wget https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-valid.txt\")\n",
    "\n",
    "        # Check if validation file exists as fallback\n",
    "        validation_path = \"/content/data/TinyStoriesV2-GPT4-valid.txt\"\n",
    "        if os.path.exists(validation_path):\n",
    "            print(f\"\\nFound validation file: {validation_path}\")\n",
    "            response = input(\"Would you like to use the validation file instead? (y/n): \")\n",
    "            if response.lower() == 'y':\n",
    "                input_path = validation_path\n",
    "                print(f\"Using validation file: {input_path}\")\n",
    "            else:\n",
    "                sys.exit(1)\n",
    "        else:\n",
    "            sys.exit(1)\n",
    "\n",
    "    try:\n",
    "        # Train BPE tokenizer with profiling\n",
    "        vocab, merges, training_time, memory_stats, profile_stats = profile_bpe_training(\n",
    "            input_path, vocab_size, special_tokens\n",
    "        )\n",
    "\n",
    "        # Analyze vocabulary\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"VOCABULARY ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        vocab_analysis = analyze_vocabulary(vocab)\n",
    "\n",
    "        print(f\"Vocabulary size: {vocab_analysis['vocab_size']}\")\n",
    "        print(f\"Longest token: {vocab_analysis['longest_token']}\")\n",
    "        print(f\"Longest token ID: {vocab_analysis['longest_token_id']}\")\n",
    "        print(f\"Longest token length: {vocab_analysis['longest_token_length']} bytes\")\n",
    "        print(f\"Longest token (as string): '{vocab_analysis['longest_token_str']}'\")\n",
    "        print(f\"Is longest token readable: {vocab_analysis['longest_token_readable']}\")\n",
    "        print(f\"Average token length: {vocab_analysis['avg_token_length']:.2f} bytes\")\n",
    "\n",
    "        # Analyze if longest token makes sense\n",
    "        print(f\"\\nLongest token analysis:\")\n",
    "        if vocab_analysis['longest_token_readable']:\n",
    "            print(f\"✓ The longest token '{vocab_analysis['longest_token_str']}' appears to be readable text\")\n",
    "            print(\"✓ This makes sense as BPE should merge frequently occurring character sequences\")\n",
    "        else:\n",
    "            print(f\"⚠ The longest token contains non-printable characters\")\n",
    "            print(\"This might indicate special characters or encoding artifacts\")\n",
    "\n",
    "        # Save results\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"SAVING RESULTS\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        save_tokenizer_results(vocab, merges, output_dir)\n",
    "\n",
    "        # Performance analysis\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PERFORMANCE ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        print(f\"Training time: {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
    "        print(f\"Memory usage:\")\n",
    "        print(f\"  - Peak memory: {memory_stats['peak_memory_mb']:.2f} MB ({memory_stats['peak_memory_mb']/1024:.2f} GB)\")\n",
    "        print(f\"  - Start memory: {memory_stats['start_memory_mb']:.2f} MB\")\n",
    "        print(f\"  - Memory increase: {memory_stats['peak_memory_mb'] - memory_stats['start_memory_mb']:.2f} MB\")\n",
    "\n",
    "        # Check if requirements are met\n",
    "        time_ok = training_time <= 30 * 60  # 30 minutes\n",
    "        memory_ok = memory_stats['peak_memory_mb'] <= 30 * 1024  # 30 GB\n",
    "\n",
    "        print(f\"\\nRequirement compliance:\")\n",
    "        print(f\"  - Time ≤ 30 minutes: {'✓' if time_ok else '✗'} ({training_time/60:.2f} min)\")\n",
    "        print(f\"  - Memory ≤ 30 GB: {'✓' if memory_ok else '✗'} ({memory_stats['peak_memory_mb']/1024:.2f} GB)\")\n",
    "\n",
    "        # Most time-consuming operations\n",
    "        print(f\"\\nTop 10 most time-consuming operations:\")\n",
    "        profile_stats.print_stats(10)\n",
    "\n",
    "        # Summary for task questions\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"TASK ANSWERS SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        print(f\"Q: How many hours and memory did training take?\")\n",
    "        print(f\"A: Training took {training_time/3600:.4f} hours ({training_time:.2f} seconds)\")\n",
    "        print(f\"   Peak memory usage was {memory_stats['peak_memory_mb']:.2f} MB ({memory_stats['peak_memory_mb']/1024:.2f} GB)\")\n",
    "\n",
    "        print(f\"\\nQ: What is the longest token in the vocabulary? Does it make sense?\")\n",
    "        print(f\"A: The longest token is '{vocab_analysis['longest_token_str']}' ({vocab_analysis['longest_token_length']} bytes)\")\n",
    "        print(f\"   This {'makes sense' if vocab_analysis['longest_token_readable'] else 'may not make complete sense'} as it represents\")\n",
    "        print(f\"   {'a common character sequence that BPE identified' if vocab_analysis['longest_token_readable'] else 'potentially special characters or encoding artifacts'}\")\n",
    "\n",
    "        print(f\"\\nQ: What part of the tokenizer training process takes the most time?\")\n",
    "        print(f\"A: Based on profiling, the most time-consuming operations are:\")\n",
    "\n",
    "        # Get top 3 functions by cumulative time\n",
    "        stats_list = []\n",
    "        profile_stats.print_stats(0)  # This populates the stats\n",
    "        for func, (_, _, _, ct, _) in profile_stats.stats.items():\n",
    "            stats_list.append((ct, func))\n",
    "        stats_list.sort(reverse=True)\n",
    "\n",
    "        for i, (cumtime, func) in enumerate(stats_list[:3]):\n",
    "            _, _, func_name = func\n",
    "            print(f\"   {i+1}. {func_name} ({cumtime:.3f}s cumulative)\")\n",
    "\n",
    "        print(f\"\\nAll results have been saved to the '{output_dir}' directory.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during execution: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
